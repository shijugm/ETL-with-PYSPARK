{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myspark=SparkSession.builder.appName(\"Spark_DF_Operations\").master(\"yarn\")\\\n",
    "    .config(\"spark.executor.memory\",\"4g\")\\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write in HDFS\n",
    "\n",
    "The data files used below is available in this repo under data folder. LOAD this file in the HDFS at /tmp directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using df.write.save()\n",
    "\n",
    "df.write.save(path=None, format=None, mode=None, partitionBy=None, **options):\n",
    "\n",
    "    path – the path in a Hadoop supported file system\n",
    "    format – the format used to save. options are 'orc','parquet','csv','text'\n",
    "    mode – specifies the behavior of the save operation when data already exists.\n",
    "        append: Append contents of this DataFrame to existing data.\n",
    "        overwrite: Overwrite existing data.\n",
    "        ignore: Silently ignore this operation if data already exists.\n",
    "        error (default case): Throw an exception if data already exists.\n",
    "    partitionBy – names of partitioning columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = myspark.read.format(\"csv\").options(header=True, inferSchema=True, sep=\",\",\n",
    "    dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\", ignoreLeadingWhiteSpace=True,\\\n",
    "    ignoreTrailingWhiteSpace=True, path=\"/tmp/sampledata.csv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"dated\", F.to_date(\"dated\").cast(T.DateType()))\\\n",
    " .withColumn(\"timing\", F.from_unixtime(F.unix_timestamp(\"timing\"),\"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+-------------------+\n",
      "|fname |lname |age|height|dated     |timing             |\n",
      "+------+------+---+------+----------+-------------------+\n",
      "|naresh|jangra|30 |170.5 |2013-10-12|2013-10-12 12:35:50|\n",
      "|ravi  |verma |35 |155.67|2014-10-12|2014-10-12 01:55:50|\n",
      "|viren |nain  |55 |160.0 |2015-10-12|2015-10-12 09:15:50|\n",
      "|bhanu |pratap|11 |180.8 |2016-10-12|2016-10-12 10:05:50|\n",
      "+------+------+---+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By Default, 200 Partitions will be created after shuffle and it will write 200 files in HDFS. \n",
    "# To Control this, set below property.\n",
    "myspark.conf.set(\"spark.sql.shuffle.partitions\",5)\n",
    "\n",
    "df.write.save(path = \"/tmp/sparkdata\", format=\"csv\", mode=\"overwrite\", partitionBy=(\"age\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [768019@pxlbig14 Production EdgeNode 19:21:21 ~]$ hadoop fs -ls /tmp/sparkdata\n",
    "    Found 5 items\n",
    "    -rw-rw-rw-   3 768019 supergroup          0 2018-02-25 19:21 /tmp/sparkdata/_SUCCESS\n",
    "    drwxrwxrwx   - 768019 supergroup          0 2018-02-25 19:21 /tmp/sparkdata/age=11\n",
    "    drwxrwxrwx   - 768019 supergroup          0 2018-02-25 19:21 /tmp/sparkdata/age=30\n",
    "    drwxrwxrwx   - 768019 supergroup          0 2018-02-25 19:21 /tmp/sparkdata/age=35\n",
    "    drwxrwxrwx   - 768019 supergroup          0 2018-02-25 19:21 /tmp/sparkdata/age=55\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. df.write.{format}()\n",
    "\n",
    "df.write.(**options)\n",
    "\n",
    "    csv(path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None)\n",
    "\n",
    "    json(path, mode=None, compression=None, dateFormat=None, timestampFormat=None)\n",
    "\n",
    "    orc(path, mode=None, partitionBy=None, compression=None)\n",
    "\n",
    "    parquet(path, mode=None, partitionBy=None, compression=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till Spark 2.1, if we write a DF HAVING SCHEMA to Hive, it gives ERROR : xyz.csv not a SequenceFile\n",
    "\n",
    "https://issues.apache.org/jira/browse/SPARK-9272\n",
    "\n",
    "df.write.saveAsTable(\"default.mytest\", format=\"csv\", mode=\"overwrite\", partitionBy=('age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate ways to do this are :\n",
    "\n",
    "### 1. When you Do not need any Partitions.\n",
    "\n",
    "Use \"CREATE TABLE db.table AS SELECT * FROM mytable\". This will create an internal table in text format having same number of columns as spark's temp view say 'mytable'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+-------------------+\n",
      "|fname |lname |age|height|dated     |timing             |\n",
      "+------+------+---+------+----------+-------------------+\n",
      "|naresh|jangra|30 |170.5 |2013-10-12|2013-10-12 12:35:50|\n",
      "|ravi  |verma |35 |155.67|2014-10-12|2014-10-12 01:55:50|\n",
      "|viren |nain  |55 |160.0 |2015-10-12|2015-10-12 09:15:50|\n",
      "|bhanu |pratap|11 |180.8 |2016-10-12|2016-10-12 10:05:50|\n",
      "+------+------+---+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "myspark.sql(\"DROP TABLE IF EXISTS default.simple_table\")\n",
    "myspark.sql(\"CREATE TABLE default.simple_table AS SELECT * FROM mytable\")\n",
    "\n",
    "# OR\n",
    "# Create the Table in Advance and use INSERT INTO/OVERWRITE\n",
    "# myspark.sql(\"INSERT OVERWRITE TABLE default.simple_table AS SELECT * FROM mytable\")\n",
    "\n",
    "myspark.sql(\"SELECT * FROM default.simple_table\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. When you need Dynamic Partitioned Hive Table.\n",
    "\n",
    "i) Create the Table in Advance and load the data using df.write.save() method using required format,partition, mode etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Dynamic partition Table default.dynamic_part_test partitioned on \"age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "myspark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "myspark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myspark.sql(\"DROP TABLE IF EXISTS default.dynamic_part_test\")\n",
    "\n",
    "myspark.sql(\"CREATE TABLE IF NOT EXISTS default.dynamic_part_test(\\\n",
    "fname string, \\\n",
    "lname string, \\\n",
    "height double, \\\n",
    "dated string, \\\n",
    "timing string )\\\n",
    "PARTITIONED BY (age int)\\\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the Data to \"/user/hive/warehouse/dynamic_part_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myspark.conf.set(\"spark.sql.shuffle.partitions\",5)\n",
    "\n",
    "df.write.save(path = \"/user/hive/warehouse/dynamic_part_test\", format=\"csv\", mode=\"overwrite\", partitionBy=(\"age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+----------+-------------------+---+\n",
      "|fname |lname |height|dated     |timing             |age|\n",
      "+------+------+------+----------+-------------------+---+\n",
      "|bhanu |pratap|180.8 |2016-10-12|2016-10-12 10:05:50|11 |\n",
      "|naresh|jangra|170.5 |2013-10-12|2013-10-12 12:35:50|30 |\n",
      "|ravi  |verma |155.67|2014-10-12|2014-10-12 01:55:50|35 |\n",
      "|viren |nain  |160.0 |2015-10-12|2015-10-12 09:15:50|55 |\n",
      "+------+------+------+----------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"MSCK REPAIR table default.dynamic_part_test\")\n",
    "myspark.sql(\"SELECT * FROM default.dynamic_part_test\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [768019@pxlbig14 Production EdgeNode 19:33:52 ~]$ hadoop fs -ls /user/hive/warehouse/dynamic_part_test\n",
    "    Found 5 items\n",
    "    -rwxrwx--x+  3 hive hive          0 2018-02-25 19:33 /user/hive/warehouse/dynamic_part_test/_SUCCESS\n",
    "    drwxrwx--x+  - hive hive          0 2018-02-25 19:33 /user/hive/warehouse/dynamic_part_test/age=11\n",
    "    drwxrwx--x+  - hive hive          0 2018-02-25 19:33 /user/hive/warehouse/dynamic_part_test/age=30\n",
    "    drwxrwx--x+  - hive hive          0 2018-02-25 19:33 /user/hive/warehouse/dynamic_part_test/age=35\n",
    "    drwxrwx--x+  - hive hive          0 2018-02-25 19:33 /user/hive/warehouse/dynamic_part_test/age=55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Create the Table in Advance and load using INSERT OVERWRITE COMMAND. Make sure the order of columns is correct and partitioned Column 'age' is used in the end of SELECT command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "myspark.conf.set(\"spark.sql.shuffle.partitions\",5)\n",
    "\n",
    "myspark.sql(\"INSERT OVERWRITE TABLE default.dynamic_part_test PARTITION(age)\\\n",
    "            SELECT fname , lname ,height ,dated,timing, age \\\n",
    "            FROM mytable\\\n",
    "            \")\n",
    "myspark.sql(\"SELECT * FROM default.dynamic_part_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. For a Static partition Table:\n",
    "    \n",
    "i) Create the Table in Advace and Save the Dataframe as Temp view or Table in spark and run myspark.sql(\"INSERT INTO hivedb.table PARTITION (name=value) select * FROM sparktable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Static partition Table default.static_part_test partitioned on \"number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "myspark.sql(\"DROP TABLE IF EXISTS default.static_part_test\")\n",
    "\n",
    "myspark.sql(\"CREATE TABLE IF NOT EXISTS default.static_part_test(\\\n",
    "fname string, \\\n",
    "lname string, \\\n",
    "age int, \\\n",
    "height double, \\\n",
    "dated date, \\\n",
    "timing string )\\\n",
    "PARTITIONED BY (number int)\\\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting Data for partition number=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+-------------------+------+\n",
      "| fname| lname|age|height|     dated|             timing|number|\n",
      "+------+------+---+------+----------+-------------------+------+\n",
      "|naresh|jangra| 30| 170.5|2013-10-12|2013-10-12 12:35:50|     1|\n",
      "|  ravi| verma| 35|155.67|2014-10-12|2014-10-12 01:55:50|     1|\n",
      "| viren|  nain| 55| 160.0|2015-10-12|2015-10-12 09:15:50|     1|\n",
      "| bhanu|pratap| 11| 180.8|2016-10-12|2016-10-12 10:05:50|     1|\n",
      "+------+------+---+------+----------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"INSERT OVERWRITE TABLE default.static_part_test PARTITION (number=1) SELECT * FROM mytable\")\n",
    "myspark.sql(\"SELECT * FROM default.static_part_test where number=1 \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting data for partition number=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+-------------------+------+\n",
      "| fname| lname|age|height|     dated|             timing|number|\n",
      "+------+------+---+------+----------+-------------------+------+\n",
      "|naresh|jangra| 30| 170.5|2013-10-12|2013-10-12 12:35:50|     2|\n",
      "+------+------+---+------+----------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"INSERT OVERWRITE TABLE default.static_part_test PARTITION (number=2) SELECT * FROM mytable where fname='naresh'\")\n",
    "myspark.sql(\"SELECT * FROM default.static_part_test where number=2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Create the Table in Advace and load the data using df.write.save() method using required format,mode etc. \n",
    "    \n",
    "Note that we can not use partitionBy here which can be done only for Dynamic partition where partitoined column is part of the table.\n",
    "    \n",
    "We will choose the complete HDFS path (/user/hive/warehouse/static_part_test/number=3) which will include the Partition details itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myspark.conf.set(\"spark.sql.shuffle.partitions\",5)\n",
    "\n",
    "df.write.save(path = \"/user/hive/warehouse/static_part_test/number=3\", format=\"csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+-------------------+------+\n",
      "| fname| lname|age|height|     dated|             timing|number|\n",
      "+------+------+---+------+----------+-------------------+------+\n",
      "|naresh|jangra| 30| 170.5|2013-10-12|2013-10-12 12:35:50|     3|\n",
      "|  ravi| verma| 35|155.67|2014-10-12|2014-10-12 01:55:50|     3|\n",
      "| viren|  nain| 55| 160.0|2015-10-12|2015-10-12 09:15:50|     3|\n",
      "| bhanu|pratap| 11| 180.8|2016-10-12|2016-10-12 10:05:50|     3|\n",
      "+------+------+---+------+----------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"MSCK REPAIR TABLE default.static_part_test\")\n",
    "myspark.sql(\"SELECT * FROM default.static_part_test where number=3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What's Next\n",
    "\n",
    "1) To Download this Single Notebook, Click this file in my Github Account, Copy the URL and paste in http://nbviewer.jupyter.org/. Download button will be in top right corner.\n",
    "\n",
    "2) Open your Juypter Notebook home page and upload using \"upload\" Button.\n",
    "\n",
    "3) Continue Learning from the next Notebook Spark_04_Date_Timestamp_Handling.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
